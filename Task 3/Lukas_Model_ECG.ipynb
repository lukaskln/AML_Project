{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#### SetUp\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import lightgbm as lgb\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\r\n",
        "from sklearn.model_selection import cross_validate, GridSearchCV, KFold\r\n",
        "from sklearn.linear_model import Lasso, LogisticRegression\r\n",
        "from sklearn.feature_selection import SelectFromModel, RFE\r\n",
        "from sklearn.svm import SVC, OneClassSVM\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "import xgboost as xgb\r\n",
        "\r\n",
        "import pywt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import pywt\r\n",
        "import seaborn as sns\r\n",
        "import scaleogram as scg \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.gridspec as GridSpec\r\n",
        "from mat4py import loadmat\r\n",
        "from scipy.fftpack import fft\r\n",
        "\r\n",
        "import plotly.express as px\r\n",
        "import plotly.graph_objects as go\r\n",
        "import plotly.figure_factory as ff\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "from scipy import stats\r\n",
        "from itertools import repeat\r\n",
        "\r\n",
        "import biosppy.signals.ecg as ecg\r\n",
        "\r\n",
        "vColors = [\"#049DD9\", \"#03A64A\", \"#F2AC29\", \"#F2CA80\", \"#F22929\"]\r\n",
        "\r\n",
        "from scipy.signal import spectrogram\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "import neurokit2 as nk\r\n",
        "from neurokit2 import ecg_quality, ecg_clean, ecg_process, ecg_analyze, ecg_plot, entropy_sample, entropy_approximate, signal_detrend, signal_power, signal_psd\r\n",
        "from joblib import Parallel, delayed\r\n",
        "from tqdm import tqdm\r\n",
        "import heartpy as hp\r\n",
        "from scipy.signal import resample\r\n",
        "import math\r\n",
        "import biosppy"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1605997957052
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.read_csv('X_train.csv').drop(labels='id', axis=1).drop(\"x17978\", axis = 1).to_numpy()\r\n",
        "y_train = np.ravel(pd.read_csv('y_train.csv').drop(labels='id', axis=1).to_numpy())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning:\n",
            "\n",
            "Columns (17979) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "\n"
          ]
        }
      ],
      "execution_count": 78,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1606000787884
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering comparision\r\n",
        "\r\n",
        "n = 15\r\n",
        "\r\n",
        "row = X_train[n,:][~np.isnan(X_train[n,100:])]\r\n",
        "\r\n",
        "X_train_filtered = biosppy.ecg.ecg(row, sampling_rate=300, show = False)[\"filtered\"]\r\n",
        "\r\n",
        "fig, axs = plt.subplots(2)\r\n",
        "axs[0].plot(row, 'tab:orange')\r\n",
        "axs[0].set_title(\"raw\")\r\n",
        "axs[1].plot(X_train_filtered, 'tab:green')\r\n",
        "axs[1].set_title(\"filtered\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1605886417850
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction loop\r\n",
        "\r\n",
        "X_train_ecg = []\r\n",
        "Fails = list()\r\n",
        "    q1\r\n",
        "for i in range(0,X_train.shape[0],1):\r\n",
        "    if i % 50 == 0:\r\n",
        "        print(i)\r\n",
        "    \r\n",
        "    try:\r\n",
        "        X_train_filtered = biosppy.ecg.ecg(X_train[i,:][~np.isnan(X_train[i,:])], sampling_rate=300, show = False)[\"filtered\"]\r\n",
        "\r\n",
        "        resampled_data = resample(X_train_filtered, len(X_train_filtered)*5)\r\n",
        "        scaled_data = hp.scale_data(resampled_data)\r\n",
        "\r\n",
        "        try:\r\n",
        "            processed_data, info = nk.ecg_process(scaled_data, sampling_rate=300)\r\n",
        "        except:\r\n",
        "            try:\r\n",
        "                processed_data, info = nk.ecg_process(-1*scaled_data, sampling_rate=300)\r\n",
        "            except:\r\n",
        "                processed_data, info = nk.ecg_process(np.square(scaled_data), sampling_rate=300)\r\n",
        "\r\n",
        "        X_train_ecg.append(nk.bio_analyze(processed_data, sampling_rate=300).iloc[0,:].to_list())\r\n",
        "    except:\r\n",
        "        Fails.append(i)\r\n",
        "        print(i,\"did not work\")\r\n",
        "\r\n",
        "\r\n",
        "X_train_ecg = pd.DataFrame(X_train_ecg, columns = list(nk.bio_analyze(processed_data, sampling_rate=300).columns))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1605996097595
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[314,\r\n",
        " 835,\r\n",
        " 976,\r\n",
        " 1097,\r\n",
        " 1640,\r\n",
        " 2106,\r\n",
        " 2888,\r\n",
        " 3348,\r\n",
        " 3933,\r\n",
        " 4165,\r\n",
        " 4239,\r\n",
        " 4970,\r\n",
        " 4980,\r\n",
        " 5072] in Train Data can not be processed!\r\n",
        "\r\n",
        "83, 1908, 2630, 2828, 2902, 3003, 3357 in Test Data can not be processed!"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For export\r\n",
        "#X_train_ecg.to_csv('X_train_ecg.csv')"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1605996543817
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import (again) and deleting not processed rows in response vector\r\n",
        "\r\n",
        "X_train_ecg = pd.read_csv(\"X_train_ecg.csv\").drop([\"HRV_ULF\",\"HRV_VLF\"], axis=1)\r\n",
        "y_train = np.delete(y_train, [314, 835, 976, 1097, 1640, 2106, 2888, 3348, 3933, 4165, 4239, 4970, 4980, 5072], axis=0).ravel()"
      ],
      "outputs": [],
      "execution_count": 79,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1606000811321
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ecg = X_train_ecg.replace([np.inf, -np.inf], np.nan)\r\n",
        "\r\n",
        "X_train_ecg.fillna(X_train_ecg.median(), inplace=True)"
      ],
      "outputs": [],
      "execution_count": 80,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1606000819243
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler() \r\n",
        "\r\n",
        "X_train_ecg = pd.DataFrame(\r\n",
        "    scaler.fit_transform(\r\n",
        "    X_train_ecg),\r\n",
        "    columns=X_train_ecg.columns)"
      ],
      "outputs": [],
      "execution_count": 85,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1606000932722
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If wanted: Feature reduction by lasso (No must)\r\n",
        "\r\n",
        "lsvc = LogisticRegression(C=0.01, penalty='l1', solver = \"liblinear\").fit(X_train_ecg, y_train)\r\n",
        "model = SelectFromModel(lsvc, prefit=True)\r\n",
        "\r\n",
        "X_train_ecg = pd.DataFrame(model.transform(X_train_ecg))\r\n",
        "\r\n",
        "X_train_ecg.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 91,
          "data": {
            "text/plain": "(5103, 22)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 91,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1606001229288
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "F1 = []\r\n",
        "cv = KFold(n_splits=5)\r\n",
        "\r\n",
        "model = RandomForestClassifier(n_estimators = 500)\r\n",
        "\r\n",
        "# model = lgb.LGBMClassifier(objective='multiclass',\r\n",
        "#                         n_estimators = 50,\r\n",
        "#                         learning_rate = 0.1,\r\n",
        "#                         random_state=42,\r\n",
        "#                         max_depth = 100,\r\n",
        "#                         min_child_samples = 2,\r\n",
        "#                         min_child_weight = 0,\r\n",
        "#                         subsample = 0.8,\r\n",
        "#                         colsample_bytree = 0.8,\r\n",
        "#                         reg_alpha = 0\r\n",
        "#                         )\r\n",
        "\r\n",
        "for train_ix, test_ix in cv.split(X_train_ecg):\r\n",
        "\r\n",
        "    X_cvtrain, X_cvtest = X_train_ecg.iloc[train_ix, :], X_train_ecg.iloc[test_ix, :]\r\n",
        "    y_cvtrain, y_cvtest = y_train[train_ix], y_train[test_ix]\r\n",
        "\r\n",
        "    model.fit(X_cvtrain, y_cvtrain)\r\n",
        "\r\n",
        "    predtrain = model.predict(X_cvtrain)\r\n",
        "    pred = model.predict(X_cvtest)\r\n",
        "\r\n",
        "    print(\"\\nTrain F1:\")\r\n",
        "    print(np.round(f1_score(y_cvtrain, predtrain, average='micro'), 4))\r\n",
        "    print(\"\\nTest F1:\")\r\n",
        "    print(np.round(f1_score(y_cvtest, pred, average='micro'), 4))\r\n",
        "    print(\"\\n________________________\")\r\n",
        "\r\n",
        "    F1.append(np.round(f1_score(y_cvtest, pred, average= \"micro\"), 4))\r\n",
        "\r\n",
        "print(\"\\nAverage F1:\", round(np.sum(F1)/5, 4))\r\n",
        "print(\"Std:\", round(np.std(F1), 4))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1606001403173
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}